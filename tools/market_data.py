"""Market data tools and workflows."""

from __future__ import annotations

import asyncio
from datetime import datetime, timedelta, timezone
import logging
import os
from pathlib import Path
from typing import Any, List

import aiohttp

from pydantic import BaseModel
from temporalio import activity, workflow
from data_loader import CCXTAPILoader, DataCache
from data_loader.utils import timeframe_to_seconds
from tools.feature_engineering import signal_compute_vector, load_historical_data
from agents.constants import STREAM_CONTINUE_EVERY, STREAM_HISTORY_LIMIT


class MarketTick(BaseModel):
    """Ticker payload sent to child workflows."""

    exchange: str
    symbol: str
    data: dict[str, Any]


# MCP server endpoint for recording tick signals
MCP_HOST = os.environ.get("MCP_HOST", "localhost")
MCP_PORT = os.environ.get("MCP_PORT", "8080")

# Historical data to load on startup (in minutes)
HISTORICAL_MINUTES = int(os.environ.get("HISTORICAL_MINUTES", "60"))  # Default 1 hour

LOG_LEVEL = os.environ.get("LOG_LEVEL", "INFO")
logging.basicConfig(level=LOG_LEVEL, format="[%(asctime)s] %(levelname)s: %(message)s")
logger = logging.getLogger(__name__)



COINBASE_ID = "coinbaseexchange"


@activity.defn
async def fetch_ticker(symbol: str) -> dict[str, Any]:
    """Return the latest ticker for ``symbol`` from Coinbase."""
    import ccxt.async_support as ccxt
    client = ccxt.coinbaseexchange()
    try:
        data = await client.fetch_ticker(symbol)
        return MarketTick(exchange=COINBASE_ID, symbol=symbol, data=data).model_dump()
    except Exception as exc:
        logger.error("Failed to fetch ticker %s:%s - %s", COINBASE_ID, symbol, exc)
        raise
    finally:
        await client.close()


_HISTORICAL_CACHE = DataCache(root=Path("data/market_cache/raw"))
_HISTORICAL_BACKEND = CCXTAPILoader(exchange_id="coinbase", cache=_HISTORICAL_CACHE)


@activity.defn
async def fetch_historical_ohlcv(symbol: str, timeframe: str = '1m', limit: int = 60) -> List[dict]:
    """Fetch historical OHLCV data for a symbol.
    
    Parameters
    ----------
    symbol:
        Trading pair (e.g., 'BTC/USD')
    timeframe:
        Candle timeframe ('1m', '5m', '15m', '1h', etc.)
    limit:
        Number of candles to fetch (default 60 = 1 hour of 1m candles)
        
    Returns
    -------
    List[dict]
        List of ticks with timestamp and price
    """
    seconds = timeframe_to_seconds(timeframe)
    end = datetime.now(tz=timezone.utc)
    start = end - timedelta(seconds=seconds * (limit + 5))

    try:
        frame = await asyncio.to_thread(
            _HISTORICAL_BACKEND.fetch_history,
            symbol,
            start,
            end,
            timeframe,
        )
    except Exception as exc:
        logger.error("Failed to fetch historical data for %s: %s", symbol, exc)
        return []

    window = frame.tail(limit)
    ticks: List[dict] = []
    for ts, row in window.iterrows():
        timestamp_ms = int(ts.timestamp() * 1000)
        close_price = float(row.close)
        tick = {
            "timestamp": timestamp_ms,
            "last": close_price,
            "bid": close_price - 0.01,
            "ask": close_price + 0.01,
            "datetime": ts.isoformat(),
        }
        ticks.append(tick)
    logger.info("Fetched %d historical candles for %s", len(ticks), symbol)
    return ticks


@activity.defn
async def record_tick(tick: dict) -> None:
    """Send tick payload to the MCP server signal log."""
    url = f"http://{MCP_HOST}:{MCP_PORT}/signal/market_tick"
    timeout = aiohttp.ClientTimeout(total=5)
    async with aiohttp.ClientSession(timeout=timeout) as session:
        try:
            await session.post(url, json=tick)
        except Exception as exc:
            logger.error("Failed to record tick: %s", exc)


@workflow.defn
class HistoricalDataLoaderWorkflow:
    """Workflow to load historical data for multiple symbols."""
    
    @workflow.run
    async def run(self, symbols: List[str]) -> None:
        """Load historical data for each symbol."""
        workflow.logger.info(f"Loading {HISTORICAL_MINUTES} minutes of historical data for {len(symbols)} symbols")
        
        # Process symbols concurrently
        tasks = []
        for symbol in symbols:
            tasks.append(
                workflow.execute_activity(
                    fetch_historical_ohlcv,
                    args=[symbol, '1m', HISTORICAL_MINUTES],
                    schedule_to_close_timeout=timedelta(seconds=60),
                )
            )
        
        # Fetch all historical data
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Historical data loading is now handled by each ComputeFeatureVector workflow
        # when it starts up, so we just log the results here
        for symbol, result in zip(symbols, results):
            if isinstance(result, Exception):
                workflow.logger.error("Failed to fetch historical data for %s: %s", symbol, result)
                continue
                
            if result:  # If we got historical ticks
                workflow.logger.info("Historical data available for %s: %d ticks", symbol, len(result))
        
        workflow.logger.info("Historical data loading completed for %d symbols", len(symbols))


@workflow.defn
class SubscribeCEXStream:
    """Periodically fetch tickers and broadcast them to children."""

    @workflow.run
    async def run(
        self,
        symbols: List[str],
        interval_sec: int = 1,
        max_cycles: int | None = None,
        continue_every: int = STREAM_CONTINUE_EVERY,
        history_limit: int = STREAM_HISTORY_LIMIT,
    ) -> None:
        """Stream tickers indefinitely, continuing as new periodically."""
        # Feature vector workflows are started lazily via an activity

        cycles = 0
        while True:
            tickers = await asyncio.gather(
                *[
                    workflow.execute_activity(
                        fetch_ticker,
                        args=[symbol],
                        schedule_to_close_timeout=timedelta(seconds=10),
                    )
                    for symbol in symbols
                ]
            )
            tasks = []
            for t in tickers:
                tasks.append(
                    workflow.start_activity(
                        record_tick,
                        t,
                        schedule_to_close_timeout=timedelta(seconds=5),
                    )
                )
                tasks.append(
                    workflow.start_activity(
                        signal_compute_vector,
                        args=[t.get("symbol"), t],
                        schedule_to_close_timeout=timedelta(seconds=5),
                    )
                )
            await asyncio.gather(*tasks)
            cycles += 1
            if max_cycles is not None and cycles >= max_cycles:
                return
            hist_len = workflow.info().get_current_history_length()
            if hist_len >= history_limit or workflow.info().is_continue_as_new_suggested():
                await workflow.continue_as_new(
                    args=[
                        symbols,
                        interval_sec,
                        max_cycles,
                        continue_every,
                        history_limit,
                    ]
                )
            if cycles >= continue_every:
                await workflow.continue_as_new(
                    args=[
                        symbols,
                        interval_sec,
                        max_cycles,
                        continue_every,
                        history_limit,
                    ]
                )
            await workflow.sleep(interval_sec)
